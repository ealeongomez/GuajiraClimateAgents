{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# 02 - RAG con ChromaDB\n",
    "\n",
    "Este notebook demuestra c√≥mo crear una base de datos vectorial a partir de un PDF\n",
    "utilizando la clase `VectorStore` y ChromaDB.\n",
    "\n",
    "## Contenido\n",
    "1. Configuraci√≥n del entorno\n",
    "2. Carga del documento PDF\n",
    "3. Divisi√≥n en chunks\n",
    "4. Creaci√≥n de embeddings y base de datos vectorial\n",
    "5. B√∫squeda sem√°ntica (ejemplo de uso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio ra√≠z del proyecto al path\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Paths importantes\n",
    "PDF_PATH = PROJECT_ROOT / \"data\" / \"pdfs\" / \"4349.pdf\"\n",
    "EMBEDDINGS_PATH = PROJECT_ROOT / \"data\" / \"embeddings\"\n",
    "\n",
    "print(f\"üìÅ Proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"üìÑ PDF: {PDF_PATH}\")\n",
    "print(f\"üíæ Embeddings: {EMBEDDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-check-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "# Verificar que las API keys est√©n configuradas\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"‚ö†Ô∏è OPENAI_API_KEY no est√° configurada. \"\n",
    "        \"Crea un archivo .env en la ra√≠z del proyecto con tu API key.\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Variables de entorno cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-pdf-header",
   "metadata": {},
   "source": [
    "## 2. Carga del documento PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pdf-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Cargar el PDF\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"üìÑ Documento cargado: {PDF_PATH.name}\")\n",
    "print(f\"üìë N√∫mero de p√°ginas: {len(documents)}\")\n",
    "print(f\"\\nüìù Preview de la primera p√°gina:\")\n",
    "print(\"-\" * 50)\n",
    "print(documents[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## 3. Divisi√≥n en chunks\n",
    "\n",
    "Dividimos el documento en chunks m√°s peque√±os para mejorar la precisi√≥n de las b√∫squedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configurar el text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Dividir los documentos\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Documento dividido en {len(chunks)} chunks\")\n",
    "print(f\"\\nüìù Ejemplo de chunk:\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-header",
   "metadata": {},
   "source": [
    "## 4. Creaci√≥n de embeddings y base de datos vectorial\n",
    "\n",
    "Usamos la clase `VectorStore` para crear la base de datos ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vectorstore-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from src.utils.vector_store import VectorStore\n",
    "\n",
    "# Inicializar el modelo de embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"  # Modelo m√°s econ√≥mico y eficiente\n",
    ")\n",
    "\n",
    "# Crear la base de datos vectorial\n",
    "# Se guardar√° autom√°ticamente en data/embeddings/climate_docs/\n",
    "vector_store = VectorStore(\n",
    "    collection_name=\"climate_docs\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"üóÑÔ∏è VectorStore inicializado\")\n",
    "print(f\"üìÇ Ubicaci√≥n: {vector_store.persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-docs-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar los documentos a la base de datos vectorial\n",
    "# Esto puede tomar unos minutos dependiendo del tama√±o del documento\n",
    "\n",
    "print(\"‚è≥ Agregando documentos a la base de datos vectorial...\")\n",
    "ids = vector_store.add_documents(chunks)\n",
    "\n",
    "print(f\"‚úÖ Se agregaron {len(ids)} chunks a la base de datos\")\n",
    "print(f\"üìä Total de documentos en la colecci√≥n: {vector_store.get_collection_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-header",
   "metadata": {},
   "source": [
    "## 5. B√∫squeda sem√°ntica (Ejemplo de uso)\n",
    "\n",
    "Ahora podemos hacer b√∫squedas sem√°nticas sobre el contenido del PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-basic-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de b√∫squeda b√°sica\n",
    "query = \"¬øCu√°l es el tema principal del documento?\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"üîç B√∫squeda: '{query}'\")\n",
    "print(f\"\\nüìã Se encontraron {len(results)} resultados relevantes:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Resultado {i} ---\")\n",
    "    print(f\"üìÑ P√°gina: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"üìù Contenido:\\n{doc.page_content[:400]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-score-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B√∫squeda con scores de similitud\n",
    "query = \"clima\"\n",
    "\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"üîç B√∫squeda: '{query}'\")\n",
    "print(f\"\\nüìä Resultados con scores de similitud:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n--- Resultado {i} (Score: {score:.4f}) ---\")\n",
    "    print(f\"üìÑ P√°gina: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"üìù Contenido:\\n{doc.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-header",
   "metadata": {},
   "source": [
    "## 6. Ejemplo de RAG completo\n",
    "\n",
    "Combinamos la b√∫squeda vectorial con un LLM para responder preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Configurar el LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Crear el retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Template para RAG\n",
    "template = \"\"\"Responde la pregunta bas√°ndote √∫nicamente en el siguiente contexto:\n",
    "\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Crear la cadena RAG\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Cadena RAG configurada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-query-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer una pregunta usando RAG\n",
    "question = \"¬øCu√°l es el tema principal del documento?\"\n",
    "\n",
    "print(f\"‚ùì Pregunta: {question}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüí¨ Respuesta:\")\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda interactiva para hacer m√°s preguntas\n",
    "# Modifica la variable 'pregunta' y ejecuta esta celda\n",
    "\n",
    "pregunta = \"Escribe aqu√≠ tu pregunta sobre el documento\"\n",
    "\n",
    "print(f\"‚ùì {pregunta}\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "respuesta = rag_chain.invoke(pregunta)\n",
    "print(f\"\\nüí¨ {respuesta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reload-header",
   "metadata": {},
   "source": [
    "## 7. Cargar base de datos existente\n",
    "\n",
    "Si ya tienes una base de datos creada, puedes cargarla sin volver a procesar el PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reload-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Cargar una base de datos existente\n",
    "# (No es necesario agregar documentos de nuevo)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from src.utils.vector_store import VectorStore\n",
    "\n",
    "# Inicializar embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Cargar la base de datos existente\n",
    "existing_store = VectorStore(\n",
    "    collection_name=\"climate_docs\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# Verificar contenido\n",
    "count = existing_store.get_collection_count()\n",
    "print(f\"üìä Documentos en la base de datos: {count}\")\n",
    "\n",
    "# Hacer b√∫squeda\n",
    "if count > 0:\n",
    "    results = existing_store.similarity_search(\"clima\", k=2)\n",
    "    print(f\"\\nüîç B√∫squeda de prueba exitosa: {len(results)} resultados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
